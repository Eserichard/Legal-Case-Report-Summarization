{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files\n",
    "\n",
    "# for each file, parse into soup\n",
    "\n",
    "    # find all catchprases\n",
    "        # for each catchprase, extract text and store in a list\n",
    "    # save list as pickle to disk\n",
    "\n",
    "    # find all sentences\n",
    "        # for each sentence, extract text and store in a list\n",
    "    # save list as pickle to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import justext\n",
    "import nltk.data\n",
    "import pandas as pd \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory to case reports\n",
    "base_dir = 'data/fulltext'\n",
    "\n",
    "# Make directory to store extracted catchphrases\n",
    "catch_directory = os.path.dirname('data/catch_text/')\n",
    "if not os.path.exists(catch_directory):\n",
    "    os.makedirs(catch_directory)\n",
    "    \n",
    "# Make directory to store extracted sentences\n",
    "sent_directory = os.path.dirname('data/sent_text/')\n",
    "if not os.path.exists(sent_directory):\n",
    "    os.makedirs(sent_directory)\n",
    "    \n",
    "# read in case files\n",
    "for num, filename in enumerate(os.scandir(base_dir)):\n",
    "    with open(filename, 'rb') as case_file:\n",
    "            # parse case file into beautiful soup\n",
    "            soup = BeautifulSoup(case_file, 'xml')\n",
    "            \n",
    "            # obtain all catchphrases in the case file\n",
    "            catchphrases = soup.find_all('catchphrase')\n",
    "            \n",
    "            # obtain all sentences in the case file\n",
    "            sentences = soup.find_all('sentence')\n",
    "            \n",
    "            # obtain all text in identified catchphrase and store them together\n",
    "            gold_catchphrases = []\n",
    "            for catchphrase in catchphrases:\n",
    "                text = catchphrase.text\n",
    "                text = re.sub(r'.*>','',text)\n",
    "                gold_catchphrases.append(text)\n",
    "                \n",
    "            # obtain all text in identified sentences and store them together\n",
    "            gold_sentences = []\n",
    "            for sentence in sentences:\n",
    "                text = sentence.text\n",
    "                text = text.strip()\n",
    "                text = text.strip('.')\n",
    "                text = text.lstrip('0123456789.- ')\n",
    "                text = re.sub(r'(( [\\d])$)|(.*>)|(\\n)','',text)\n",
    "                text = text.strip()\n",
    "                gold_sentences.append(text)\n",
    "            \n",
    "            num += 1\n",
    "            num = str(num)\n",
    "            \n",
    "            # Removing empty sentences\n",
    "            gold_sentences = [x for x in gold_sentences if x]\n",
    "            #print(gold_sentences)\n",
    "                        \n",
    "            # save catchphrases as pickle\n",
    "            pickle_out = open(os.path.join(catch_directory, num),\"wb\")\n",
    "            pickle.dump(gold_catchphrases, pickle_out)\n",
    "            pickle_out.close()\n",
    "            \n",
    "            # save sentences as pickle\n",
    "            pickle_out = open(os.path.join(sent_directory, num),\"wb\")\n",
    "            pickle.dump(gold_sentences, pickle_out)\n",
    "            pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sample sent_text files and catch_text files\n",
    "# c = pd.read_pickle(\"data/sent_text/2\")\n",
    "# c = pd.read_pickle(\"data/catch_text/2\")\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for extracting tokens from catchphrases text and sentences text\n",
    "def tokenextraction(catch_text, sent_text):\n",
    "    catch_tokens = []\n",
    "    sent_tokens = []\n",
    "    for text in catch_text :\n",
    "        catch_tokens += treebank_tokenizer.tokenize(text)\n",
    "    for l in sent_text :\n",
    "        sent_tokens += treebank_tokenizer.tokenize(text)\n",
    "    return catch_tokens, sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating through all the pickle files for extracting tokens\n",
    "\n",
    "all_catch_tokens = []\n",
    "all_sent_tokens = []\n",
    "for i in range(1,3890):\n",
    "    try :\n",
    "        catch_text = pd.read_pickle(\"data/catch_text/\"+str(i))\n",
    "        sent_text = pd.read_pickle(\"data/sent_text/\"+str(i))\n",
    "    except EOFError:\n",
    "        catch_text = list()\n",
    "    catch_tokens, sent_tokens = tokenextraction(catch_text, sent_text)\n",
    "    all_catch_tokens.extend(catch_tokens)\n",
    "    all_sent_tokens.extend(sent_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_catch_tokens)\n",
    "# print(all_sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the tokens function\n",
    "def cleanedtokens(tokens):\n",
    "    cleaned_tokens = [w for w in tokens if w.isalnum()]\n",
    "    cleaned_tokens = [w for w in cleaned_tokens if not w in stop_words]\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the tokens\n",
    "all_catch_cleaned_tokens = cleanedtokens(all_catch_tokens)\n",
    "# print(all_catch_cleaned_tokens)\n",
    "all_sent_cleaned_tokens = cleanedtokens(all_sent_tokens)\n",
    "# print(all_sent_cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Extraction and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory to citations class\n",
    "base_dir = 'data/citations_class'\n",
    "\n",
    "# Make directory to store extracted citations\n",
    "citation_directory = os.path.dirname('data/citation_text/')\n",
    "if not os.path.exists(citation_directory):\n",
    "    os.makedirs(citation_directory)\n",
    "    \n",
    "# read in citation files\n",
    "for num, filename in enumerate(os.scandir(base_dir)):\n",
    "    with open(filename, 'rb') as case_file:\n",
    "            # parse citation file into beautiful soup\n",
    "            soup = BeautifulSoup(case_file, 'xml')\n",
    "            \n",
    "            # obtain all citations in the citations file\n",
    "            citations = soup.find_all('name')\n",
    "            \n",
    "            # obtain all cited cases in the citations file\n",
    "            cited = soup.find_all('tocase')\n",
    "            \n",
    "            # obtain all text in identified citations and store them together\n",
    "            gold_citations = []\n",
    "            for citation in citations:\n",
    "                text = citation.text\n",
    "                # Cleaning citation part by removing all the years and courts mentioned\n",
    "                text = re.sub(r'\\([^)]*\\)|(\\[[^)]*\\])|(FCA)|(\\d+)', '', text)\n",
    "                text = text.strip()\n",
    "                gold_citations.append(text)\n",
    "            \n",
    "            # obtain all text in identified cited citations and store them together\n",
    "            gold_cited = []\n",
    "            for citation in cited:\n",
    "                text = citation.text\n",
    "                # Cleaning citation part by removing all the years and courts mentioned\n",
    "                text = re.sub(r'\\([^)]*\\)|(\\[[^)]*\\])|(\\d+)|(FCA)|(;)', '', text)\n",
    "                text = re.sub(r'[ ]{2,}',' ',text)\n",
    "                text = text.strip()\n",
    "                text = re.sub(r'[A-Z]{2,}$','',text)\n",
    "                text = text.strip()\n",
    "                text = re.sub(r'[A-Z]{2,}$','',text)\n",
    "                text = text.strip()\n",
    "                text = re.sub(r'[A-Z]{2,}$','',text)\n",
    "                text = text.strip()\n",
    "                gold_cited.append(text)\n",
    "            \n",
    "            num += 1\n",
    "            num = str(num)\n",
    "            \n",
    "            # add them together\n",
    "            gold_citations = gold_citations + gold_cited\n",
    "            gold_citations = [x for x in gold_citations if x]\n",
    "            \n",
    "            # save citations as pickle\n",
    "            pickle_out = open(os.path.join(citation_directory, num),\"wb\")\n",
    "            pickle.dump(gold_citations, pickle_out)\n",
    "            pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sample citation_text file\n",
    "c = pd.read_pickle(\"data/citation_text/13\")\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
