{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import justext\n",
    "import nltk.data\n",
    "import pandas as pd \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from infomap import Infomap\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "- Set working directory\n",
    "- Create a new directory that stores the cleaned text - 'data_directory'\n",
    "- Read and parse through case files using BeautifulSoup\n",
    "- Create 3 dictionaries for multilevel storage\n",
    "- Extract and clean the Name, Catchphrases and Sentences of each case \n",
    "- In gold_catch{}, key - 'Catchphrases', value - the extracted catchphrases\n",
    "- In gold_sent{}, key - 'Sentences', value - the extracted sentences\n",
    "- In gold_text{}, key - name of case, value - gold_catch and gold_sent\n",
    "- Store each gold_text in folder 'data' as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory to case reports\n",
    "base_dir = 'data/fulltext'\n",
    "\n",
    "# Make directory to store extracted text\n",
    "cleaned_directory = os.path.dirname('data/cleanedtext/')\n",
    "if not os.path.exists(cleaned_directory):\n",
    "    os.makedirs(cleaned_directory)\n",
    "    \n",
    "# read in case files\n",
    "for num, filename in enumerate(os.scandir(base_dir)):\n",
    "    with open(filename, 'rb') as case_file:\n",
    "            # parse case file into beautiful soup\n",
    "            soup = BeautifulSoup(case_file, 'xml')\n",
    "            \n",
    "            # create 2 dictionaries to store the case names as key \n",
    "            # and list of sentences and catchphrases as values\n",
    "            # the third gold_text{} will store the other two in a multilevel fashion\n",
    "            gold_text = {}\n",
    "            gold_sent = {}\n",
    "            gold_catch = {}\n",
    "            \n",
    "            # obtain all names in the case files\n",
    "            names = soup.find_all('name')\n",
    "\n",
    "            # obtain all catchphrases in the case file\n",
    "            catchphrases = soup.find_all('catchphrase')\n",
    "            \n",
    "            # obtain all sentences in the case file\n",
    "            sentences = soup.find_all('sentence')\n",
    "            \n",
    "            # clean and store the extracted names\n",
    "            for name in names:\n",
    "                na = name.text\n",
    "                na = re.sub(r'\\([^)]*\\)|(\\[[^)]*\\])|(FCA)|(\\d+)', '', na)\n",
    "                na = na.strip()    \n",
    "#                 print(na)\n",
    "                \n",
    "            # clean and store the extracted catchphrases \n",
    "            gold_catchphrases = []\n",
    "            for catchphrase in catchphrases:\n",
    "                text = catchphrase.text\n",
    "                text = re.sub(r'.*>','',text)\n",
    "                gold_catchphrases.append(text)\n",
    "                \n",
    "            # clean and store the extracted sentences                \n",
    "            gold_sentences = []\n",
    "            for sentence in sentences:\n",
    "                text = sentence.text\n",
    "                text = text.strip()\n",
    "                text = text.strip('.')\n",
    "                text = text.lstrip('0123456789.- ')\n",
    "                text = re.sub(r'(( [\\d])$)|(.*>)|(\\n)','',text)\n",
    "                text = text.strip()\n",
    "                gold_sentences.append(text)\n",
    "                \n",
    "            num += 1\n",
    "            num = str(num)\n",
    "                \n",
    "            # remove any missing sentences\n",
    "            gold_sentences = [x for x in gold_sentences if x]\n",
    "\n",
    "            # store the name as key and the sentences as value\n",
    "            gold_sent['Sentences'] = gold_sentences\n",
    "            \n",
    "            # store the name as key and the catchphrases as value\n",
    "            gold_catch['Catchphrases'] = gold_catchphrases\n",
    "            \n",
    "            # store the sentence and chatchphrase dictionaries as sub-dictionaries\n",
    "            gold_text[na] = [gold_catch, gold_sent]     \n",
    "            \n",
    "            # save the final dictionary for each case as pickle file\n",
    "            pickle_out = open(os.path.join(cleaned_directory, num),\"wb\")\n",
    "            pickle.dump(gold_text, pickle_out)\n",
    "            pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking sample sent_text files and catch_text files\n",
    "# c = pd.read_pickle(\"data/cleanedtext/9\")\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Extraction and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory to citations class\n",
    "base_dir = 'data/citations_class'\n",
    "\n",
    "# Make directory to store extracted citations\n",
    "citation_directory = os.path.dirname('data/citation_text/')\n",
    "if not os.path.exists(citation_directory):\n",
    "    os.makedirs(citation_directory)\n",
    "    \n",
    "# read in citation files\n",
    "for num, filename in enumerate(os.scandir(base_dir)):\n",
    "    with open(filename, 'rb') as case_file:\n",
    "            # parse citation file into beautiful soup\n",
    "            soup = BeautifulSoup(case_file, 'xml')\n",
    "            \n",
    "            # obtain all citations in the citations file\n",
    "            citations = soup.find_all('name')\n",
    "            \n",
    "            # obtain all cited cases in the citations file\n",
    "            cited = soup.find_all('tocase')\n",
    "            \n",
    "            # obtain all text in identified citations and store them together\n",
    "            gold_citations = []\n",
    "            for citation in citations:\n",
    "                text = citation.text\n",
    "                # Cleaning citation part by removing all the years and courts mentioned\n",
    "                text = re.sub(r'\\([^)]*\\)|(\\[[^)]*\\])|(FCA)|(\\d+)', '', text)\n",
    "                text = text.strip()\n",
    "                gold_citations.append(text)\n",
    "            \n",
    "            # obtain all text in identified cited citations and store them together\n",
    "            gold_cited = []\n",
    "            for citation in cited:\n",
    "                text = citation.text\n",
    "                # Cleaning citation part by removing all the years and courts mentioned\n",
    "                text = re.sub(r'\\([^)]*\\)|(\\[[^)]*\\])|(\\d+)|(FCA)|(;)', '', text)\n",
    "                text = re.sub(r'[ ]{2,}',' ',text)\n",
    "                text = text.strip()\n",
    "                text = re.sub(r'[A-Z]{2,}$','',text)\n",
    "                text = text.strip()\n",
    "                text = re.sub(r'[A-Z]{2,}$','',text)\n",
    "                text = text.strip()\n",
    "                text = re.sub(r'[A-Z]{2,}$','',text)\n",
    "                text = text.strip()\n",
    "                gold_cited.append(text)\n",
    "            \n",
    "            num += 1\n",
    "            num = str(num)\n",
    "            \n",
    "            # add them together\n",
    "            gold_citations = gold_citations + gold_cited\n",
    "            \n",
    "            # save citations as pickle\n",
    "            pickle_out = open(os.path.join(citation_directory, num),\"wb\")\n",
    "            pickle.dump(gold_citations, pickle_out)\n",
    "            pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sample citation_text file\n",
    "# c = pd.read_pickle(\"data/citation_text/83\")\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering cases based on Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_dict = {}\n",
    "\n",
    "for i in range(1,2755):\n",
    "    citation_text = pd.read_pickle(\"data/citation_text/\"+str(i))\n",
    "    if citation_text[0] not in citation_dict.keys():\n",
    "        citation_dict[citation_text[0]] = citation_text[1:]\n",
    "    else :\n",
    "        citation_dict[citation_text[0]].extend(citation_text[1:])\n",
    "\n",
    "# dictionary to assign numbers to cases i.e numbers as keys and case names as values\n",
    "num_dict = {}\n",
    "# dictionary to assign cases to numbers i.e case names as keys and numbers as values\n",
    "dict_num = {}\n",
    "dict_keys = list(citation_dict.keys())\n",
    "dict_values = list(citation_dict.values())\n",
    "dict_values = [item for sublist in dict_values for item in sublist]\n",
    "citation_list = list(set(dict_keys + dict_values))\n",
    "for i in range(len(citation_list)):\n",
    "    num_dict[i] = citation_list[i]\n",
    "    dict_num[citation_list[i]] = i\n",
    "\n",
    "\n",
    "# dictionary with case numbers instead of case names (for infomap)\n",
    "citation_dict_num = {}\n",
    "for i in range(len(num_dict.keys())):\n",
    "    if num_dict[i] in dict_keys:\n",
    "        citation_dict_num[i] = []\n",
    "        for j in range(len(citation_dict[num_dict[i]])):\n",
    "            if i in citation_dict_num.keys():\n",
    "                citation_dict_num[i].append(dict_num[citation_dict[num_dict[i]][j]])\n",
    "            else :\n",
    "                citation_dict_num[i] = dict_num[citation_dict[num_dict[i]][j]]\n",
    "                \n",
    "\n",
    "im = Infomap()\n",
    "\n",
    "# Creating nodes and links between nodes for infomap\n",
    "for i in citation_dict_num.keys():\n",
    "    for j in range(len(citation_dict_num[i])):\n",
    "        im.add_link(i,citation_dict_num[i][j])\n",
    "\n",
    "        \n",
    "# Run the Infomap search algorithm to find optimal modules\n",
    "im.run()\n",
    "        \n",
    "modules_dict = im.get_modules()\n",
    "\n",
    "# dictionary with clusters as keys and case numbers as values\n",
    "cluster_dict = {}\n",
    "for node in im.tree:\n",
    "    if node.is_leaf:\n",
    "        if num_dict[node.node_id] in dict_keys:\n",
    "            if node.module_id in cluster_dict.keys():\n",
    "                cluster_dict[node.module_id].append(node.node_id)\n",
    "            else:\n",
    "                cluster_dict[node.module_id] = [node.node_id]\n",
    "\n",
    "\n",
    "# dictionary with clusters as keys and case numbers as values (clusters with >=10 cases)\n",
    "top_clusters_dict = {}\n",
    "for key,value in cluster_dict.items():\n",
    "    #print(key, len(value))\n",
    "    if len(value) >= 10:\n",
    "        top_clusters_dict[key] = value\n",
    "\n",
    "\n",
    "# dictionary with keys as cluster number and values as names of cases\n",
    "top_clusters_dict_name = {}\n",
    "for key in top_clusters_dict.keys():\n",
    "    top_clusters_dict_name[key] = []\n",
    "    for value in top_clusters_dict[key]:\n",
    "        if key in top_clusters_dict_name.keys():\n",
    "            top_clusters_dict_name[key].append(num_dict[value])\n",
    "        else:\n",
    "            top_clusters_dict_name[key] = num_dict[value]\n",
    "            \n",
    "\n",
    "# Saving the dictionary as pickle file\n",
    "cluster_directory = os.path.dirname('data/clusterdata/')\n",
    "if not os.path.exists(cluster_directory):\n",
    "    os.makedirs(cluster_directory)\n",
    "\n",
    "pickle_out = open(os.path.join(cluster_directory, 'clusters'),\"wb\")\n",
    "pickle.dump(top_clusters_dict_name, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking clusters pickle file\n",
    "# c = pd.read_pickle(\"data/clusterdata/clusters\")\n",
    "# print(c)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
